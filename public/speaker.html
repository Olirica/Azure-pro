<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <title>Simo by Cano√´ ¬∑ Speaker</title>
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <link rel="icon" type="image/svg+xml" href="/favicon.svg" />
    <style>
      :root {
        color-scheme: light dark;
        font-family: system-ui, -apple-system, BlinkMacSystemFont, "Segoe UI", sans-serif;
        line-height: 1.5;
      }
      body {
        margin: 0;
        padding: 1.5rem;
        background: #0f172a;
        color: #f1f5f9;
      }
      main {
        max-width: 960px;
        margin: 0 auto;
      }
      section {
        background: rgba(15, 23, 42, 0.6);
        border-radius: 12px;
        padding: 1.5rem;
        margin-bottom: 1.5rem;
        box-shadow: 0 25px 30px -24px rgba(15, 23, 42, 0.8);
        border: 1px solid rgba(148, 163, 184, 0.2);
      }
      h1,
      h2 {
        margin-top: 0;
      }
      label {
        display: block;
        margin-top: 0.75rem;
        font-weight: 600;
      }
      input,
      textarea {
        width: 100%;
        padding: 0.6rem 0.8rem;
        margin-top: 0.35rem;
        border-radius: 8px;
        border: 1px solid rgba(148, 163, 184, 0.3);
        background: rgba(15, 23, 42, 0.35);
        color: inherit;
      }
      button {
        display: inline-flex;
        align-items: center;
        gap: 0.5rem;
        padding: 0.6rem 1rem;
        border-radius: 8px;
        border: none;
        font-weight: 600;
        cursor: pointer;
        margin-right: 1rem;
        background: #38bdf8;
        color: #0f172a;
        transition: transform 120ms ease, box-shadow 120ms ease;
      }
      button[disabled] {
        opacity: 0.5;
        cursor: not-allowed;
      }
      button:hover:not([disabled]) {
        transform: translateY(-1px);
        box-shadow: 0 10px 18px -10px rgba(56, 189, 248, 0.9);
      }
      ul {
        list-style: none;
        padding: 0;
        margin: 0;
      }
      li {
        padding: 0.75rem 1rem;
        margin-bottom: 0.5rem;
        border-radius: 8px;
        border: 1px solid rgba(148, 163, 184, 0.25);
        background: rgba(15, 23, 42, 0.3);
      }
      li[data-stage="soft"] {
        border-color: rgba(94, 234, 212, 0.2);
      }
      li[data-stage="hard"] {
        border-color: rgba(186, 230, 253, 0.45);
        background: rgba(14, 116, 144, 0.2);
      }
      .status {
        display: inline-flex;
        align-items: center;
        gap: 0.35rem;
        font-size: 0.95rem;
        margin-right: 1.25rem;
      }
      .badge {
        padding: 0.15rem 0.55rem;
        border-radius: 999px;
        font-size: 0.75rem;
        text-transform: uppercase;
        letter-spacing: 0.07em;
        background: rgba(56, 189, 248, 0.2);
        border: 1px solid rgba(56, 189, 248, 0.4);
      }
      .stack {
        display: flex;
        flex-wrap: wrap;
        gap: 1rem;
        align-items: center;
      }
      .stack input {
        max-width: 220px;
      }
      .stack label {
        margin-top: 0;
      }
      pre {
        background: rgba(15, 23, 42, 0.6);
        padding: 1rem;
        border-radius: 8px;
        overflow-x: auto;
        border: 1px solid rgba(148, 163, 184, 0.18);
      }
      small {
        color: rgba(148, 163, 184, 0.8);
      }
      /* Hide recognition mode for production (dev/testing only) */
      .dev-only {
        display: none;
      }
    </style>
  </head>
  <body>
    <main>
      <section>
        <h1>Speaker Console</h1>
        <p>Start your microphone capture, share the live transcript, and let listeners follow along.</p>
        <div class="stack">
          <div>
            <label for="roomId">Room ID</label>
            <input id="roomId" type="text" value="demo-room" />
          </div>
          <div>
            <label for="sourceLang">Source language (BCP-47)</label>
            <input id="sourceLang" type="text" value="en-US" />
          </div>
          <div>
            <label for="targetLangs">Target languages (comma separated)</label>
            <input id="targetLangs" type="text" value="fr-CA" />
          </div>
          <div>
            <label for="micSelect">Microphone</label>
            <select id="micSelect" style="width: 100%; padding: 0.6rem 0.8rem; margin-top: 0.35rem; border-radius: 8px; border: 1px solid rgba(148, 163, 184, 0.3); background: rgba(15, 23, 42, 0.35); color: inherit;">
              <option value="">Default microphone</option>
            </select>
          </div>
          <!-- Recognition Mode (hidden in production - remove 'dev-only' class to show) -->
          <div class="dev-only">
            <label>Recognition Mode</label>
            <div style="margin-top: 0.5rem; display: flex; gap: 1.5rem; flex-wrap: wrap;">
              <label style="display: flex; align-items: center; gap: 0.5rem; cursor: pointer;">
                <input type="radio" name="recMode" value="conversation" style="cursor: pointer;" />
                <span>Conversation (Fast VAD)</span>
              </label>
              <label style="display: flex; align-items: center; gap: 0.5rem; cursor: pointer;">
                <input type="radio" name="recMode" value="balanced" checked style="cursor: pointer;" />
                <span>Balanced (Moderate Speed)</span>
              </label>
              <label style="display: flex; align-items: center; gap: 0.5rem; cursor: pointer;">
                <input type="radio" name="recMode" value="dictation" style="cursor: pointer;" />
                <span>Dictation (Semantic)</span>
              </label>
            </div>
          </div>
        </div>
        <div class="stack" style="margin-top: 1.25rem">
          <div class="status">
            <span class="badge" id="recognitionStatus">Idle</span>
            <span id="telemetry">‚Äî</span>
          </div>
          <button id="startBtn">Start capture</button>
          <button id="stopBtn" disabled>Stop</button>
        </div>
      </section>

      <section>
        <h2>Live transcript</h2>
        <ul id="transcript"></ul>
      </section>

      <section>
        <details>
          <summary style="cursor: pointer; font-size: 1.5rem; font-weight: 600; margin-bottom: 1rem;">
            Debug
          </summary>
          <button id="copyDebugBtn" style="margin-bottom: 1rem; background: rgba(148, 163, 184, 0.2); color: inherit;">
            Copy debug log
          </button>
          <pre id="debugLog">Waiting‚Ä¶</pre>
          <small>Token acquisition, SDK events, and merge decisions appear here.</small>
        </details>
      </section>
    </main>

    <script src="https://aka.ms/csspeech/jsbrowserpackageraw"></script>
    <script>
      (() => {
        // Check for dev mode URL parameter (?dev=true)
        const urlParams = new URLSearchParams(window.location.search);
        if (urlParams.get('dev') === 'true') {
          // Show dev-only elements (recognition mode selector)
          document.querySelectorAll('.dev-only').forEach(el => {
            el.style.display = 'block';
          });
        }

        const logArea = document.getElementById('debugLog');
        const transcriptList = document.getElementById('transcript');
        const recognitionStatus = document.getElementById('recognitionStatus');
        const telemetry = document.getElementById('telemetry');
        const startBtn = document.getElementById('startBtn');
        const stopBtn = document.getElementById('stopBtn');
        const copyDebugBtn = document.getElementById('copyDebugBtn');

        const state = {
          recognizer: null,
          ws: null,
          sessionId: null,
          currentUtteranceId: null,  // Stable ID for current utterance
          rev: 0,  // Revision counter (increments within same utteranceId)
          // Partials disabled - removed: lastSoftEmitAt, lastSoftText, lastPartialFull, prefix, fastFinals
          roomId: null,
          sourceLang: null,
          targetLangs: [],
          recognitionMode: 'balanced',  // User-selected mode (conversation, balanced, or dictation)
          heartbeatTimer: null,
          heartbeatIntervalMs: 2500,
          itemsByUnit: new Map(),
          config: null,
          tokenTimer: null,
          tokenRegion: null,
          finalTimer: null,
          pendingFinals: [],  // Queue of finals waiting to be emitted (accumulate during debounce)
          maxBatchSize: 5,     // Max finals to batch before forcing emit
          finalBatchStartedAt: 0,  // Timestamp when first final in batch arrived
          phraseList: null,
          phraseHints: [],
          autoDetectLangs: [],
          patchSourceLang: null
        };

        const DEFAULT_CONFIG = {
          stablePartials: 4,
          segmentationSilenceMs: 500,
          initialSilenceMs: 3000,
          endSilenceMs: 350,
          softThrottleMs: 1000,
          softMinDeltaChars: 18,
          speechTokenRefreshMs: 540000,
          wsPingIntervalMs: 30000,
          ttsMaxBacklogSec: 8,
          ttsResumeBacklogSec: 4,
          ttsRateBoostPercent: 10,
          finalDebounceMs: 180,
          maxUtteranceDurationMs: 9000,
          phraseHints: [],
          autoDetectLangs: []
        };

        // Fast-finals defaults (effectively disabled with semantic segmentation)
        const DEFAULT_FAST_FINALS = {
          stableK: 99,           // Effectively disabled
          minStableMs: 9999,     // Effectively disabled
          minChars: 999,         // Effectively disabled
          minWords: 99,          // Effectively disabled
          emitThrottleMs: 9999,  // Effectively disabled
          punctStableMs: 9999,   // Effectively disabled
          tailGuardChars: 0,     // Not applicable
          tailGuardWords: 0      // Not applicable
        };

        async function loadConfig() {
          if (state.config) {
            return state.config;
          }
          try {
            const res = await fetch('/api/config', { cache: 'no-store' });
            if (!res.ok) {
              throw new Error(`Config request failed (${res.status})`);
            }
            const data = await res.json();
            const speechTunables =
              data && typeof data.speechTunables === 'object' && data.speechTunables
                ? data.speechTunables
                : {};
            const mergedConfig = { ...DEFAULT_CONFIG, ...data };
            const coerce = (value, fallback) => {
              const num = Number(value);
              return Number.isFinite(num) ? num : fallback;
            };
            mergedConfig.stablePartials = coerce(
              data.stablePartials ?? speechTunables.stablePartials,
              DEFAULT_CONFIG.stablePartials
            );
            mergedConfig.segmentationSilenceMs = coerce(
              data.segmentationSilenceMs ?? speechTunables.segmentationSilenceMs,
              DEFAULT_CONFIG.segmentationSilenceMs
            );
            mergedConfig.initialSilenceMs = coerce(
              data.initialSilenceMs ?? speechTunables.initialSilenceMs,
              DEFAULT_CONFIG.initialSilenceMs
            );
            mergedConfig.endSilenceMs = coerce(
              data.endSilenceMs ?? speechTunables.endSilenceMs,
              DEFAULT_CONFIG.endSilenceMs
            );
            mergedConfig.ttsMaxBacklogSec = coerce(
              data.ttsMaxBacklogSec,
              DEFAULT_CONFIG.ttsMaxBacklogSec
            );
            mergedConfig.ttsResumeBacklogSec = coerce(
              data.ttsResumeBacklogSec,
              DEFAULT_CONFIG.ttsResumeBacklogSec
            );
            mergedConfig.ttsRateBoostPercent = coerce(
              data.ttsRateBoostPercent,
              DEFAULT_CONFIG.ttsRateBoostPercent
            );
            const incomingFastFinals =
              data && typeof data.fastFinals === 'object' && data.fastFinals ? data.fastFinals : {};
            const fastFinals = {
              stableK: coerce(incomingFastFinals.stableK, DEFAULT_FAST_FINALS.stableK),
              minStableMs: coerce(incomingFastFinals.minStableMs, DEFAULT_FAST_FINALS.minStableMs),
              minChars: coerce(incomingFastFinals.minChars, DEFAULT_FAST_FINALS.minChars),
              minWords: coerce(incomingFastFinals.minWords, DEFAULT_FAST_FINALS.minWords),
              emitThrottleMs: coerce(
                incomingFastFinals.emitThrottleMs,
                DEFAULT_FAST_FINALS.emitThrottleMs
              ),
              punctStableMs: coerce(
                incomingFastFinals.punctStableMs,
                DEFAULT_FAST_FINALS.punctStableMs,
                0
              ),
              tailGuardChars: coerce(
                incomingFastFinals.tailGuardChars,
                DEFAULT_FAST_FINALS.tailGuardChars,
                0
              ),
              tailGuardWords: coerce(
                incomingFastFinals.tailGuardWords,
                DEFAULT_FAST_FINALS.tailGuardWords,
                0
              )
            };
            mergedConfig.fastFinals = { ...fastFinals };
            state.config = mergedConfig;
            state.fastFinals = fastFinals;
            state.phraseHints = Array.isArray(state.config.phraseHints)
              ? state.config.phraseHints
              : [];
            if (!state.phraseHints.some((hint) => hint.trim().toLowerCase() === 'session')) {
              state.phraseHints.push('session');
            }
            state.autoDetectLangs = Array.isArray(state.config.autoDetectLangs)
              ? state.config.autoDetectLangs
              : [];
            log('Runtime config loaded', state.config);
          } catch (err) {
            state.config = { ...DEFAULT_CONFIG, fastFinals: { ...DEFAULT_FAST_FINALS } };
            state.fastFinals = { ...DEFAULT_FAST_FINALS };
            state.phraseHints = [];
            state.autoDetectLangs = [];
            log('Failed to load config, falling back to defaults.', { error: err?.message });
          }
          const heartbeat = state.config.wsPingIntervalMs || DEFAULT_CONFIG.wsPingIntervalMs;
          state.heartbeatIntervalMs = Math.max(Math.floor(heartbeat / 3), 2000);
          return state.config;
        }

        function log(message, context = {}) {
          // Compact timestamp: HH:MM:SS.mmm
          const now = new Date();
          const timestamp = now.toTimeString().slice(0, 8) + '.' + now.getMilliseconds().toString().padStart(3, '0');
          const entry = `[${timestamp}] ${message}${
            Object.keys(context).length ? ` ${JSON.stringify(context)}` : ''
          }`;
          if (logArea.textContent === 'Waiting‚Ä¶') {
            logArea.textContent = entry;
          } else {
            logArea.textContent += `\n${entry}`;
          }
          logArea.scrollTop = logArea.scrollHeight;
          console.debug('[speaker]', message, context);
        }

        function setStatus(text) {
          recognitionStatus.textContent = text;
        }

        function setTelemetry(text) {
          telemetry.textContent = text;
        }

        function normalize(text) {
          return (text || '')
            .toLowerCase()
            .replace(/[‚Äú‚Äù"']/g, '')
            .replace(/[\p{P}\p{S}]+/gu, ' ')
            .replace(/\s+/g, ' ')
            .trim();
        }

        function dedupeContinuation(previous, incoming) {
          const next = (incoming || '').trim();
          if (!previous) {
            return next;
          }
          const prev = previous.trim();
          if (!prev) {
            return next;
          }
          if (next.startsWith(prev)) {
            return next;
          }
          const normPrev = normalize(prev);
          const normNext = normalize(next);
          if (!normPrev || !normNext) {
            return next;
          }
          let i = 0;
          while (i < normPrev.length && i < normNext.length && normPrev[i] === normNext[i]) {
            i += 1;
          }
          const overlapRatio = i / Math.max(normPrev.length, 1);
          if (overlapRatio >= 0.8) {
            return `${prev}${next.slice(i)}`;
          }
          return next;
        }

        function parseTargetLangs(value) {
          return value
            .split(',')
            .map((lang) => lang.trim())
            .filter(Boolean);
        }

        function splitSentences(text) {
          if (!text) {
            return [];
          }

          // Common abbreviations that don't end sentences (English + French)
          const ABBREVIATIONS = [
            'Dr', 'Mr', 'Mrs', 'Ms', 'Prof', 'Sr', 'Jr', 'vs', 'etc', 'Ph.D', 'M.D',
            'Mme', 'Mlle', 'M'  // French: Madame, Mademoiselle, Monsieur
          ];

          // Create regex pattern for abbreviations followed by period
          // (e.g., "Dr." shouldn't split)
          const abbrevPattern = new RegExp(
            `\\b(${ABBREVIATIONS.join('|')})\\.(?!\\s+[A-Z])`,
            'g'
          );

          // Temporarily replace abbreviations to protect them from splitting
          const ABBREV_PLACEHOLDER = '\x00ABBREV\x00';
          const protectedText = text.replace(abbrevPattern, `$1${ABBREV_PLACEHOLDER}`);

          // Split on sentence terminators (.!?) followed by space or end
          const matches = protectedText.match(/[^.!?]+[.!?]+(?:\s+|$)|[^.!?]+$/g);
          if (!matches) {
            return [text.trim()];
          }

          // Restore abbreviations and filter/merge fragments
          const sentences = matches.map((s) => s.trim().replace(new RegExp(ABBREV_PLACEHOLDER, 'g'), '.')).filter(Boolean);
          const filtered = [];

          for (let i = 0; i < sentences.length; i++) {
            const sentence = sentences[i];
            const startsWithLowercase = /^[a-z]/.test(sentence);

            // If it starts with lowercase and isn't the first sentence,
            // merge it with the next sentence (or previous if it's last)
            if (startsWithLowercase && sentences.length > 1) {
              // Try to merge with next sentence if available
              if (i + 1 < sentences.length) {
                sentences[i + 1] = sentence + ' ' + sentences[i + 1];
                continue; // Skip adding this fragment
              }
              // Otherwise merge with previous
              else if (filtered.length > 0) {
                filtered[filtered.length - 1] += ' ' + sentence;
                continue;
              }
            }

            filtered.push(sentence);
          }

          return filtered;
        }

        function computeTimestamps(result) {
          if (!result) return undefined;
          const offsetMs = Math.floor(result.offset / 10000);
          const durationMs = Math.floor(result.duration / 10000);
          return { t0: offsetMs, t1: offsetMs + durationMs };
        }

        // Generate ULID-style utteranceId (timestamp + random)
        function generateUtteranceId() {
          const timestamp = Date.now().toString(36).toUpperCase();
          const random = Math.random().toString(36).substring(2, 10).toUpperCase();
          return `U${timestamp}${random}`;
        }

        // ===== FAST-FINALS & PARTIALS CODE REMOVED =====
        // With semantic segmentation firing on punctuation, fast-finals are no longer needed.
        // SDK finals arrive within 2-3s with high quality, making partials/fast-finals unnecessary.

        function countWords(text) {
          if (!text) {
            return 0;
          }
          const tokens = text.trim().match(/\S+/g);
          return tokens ? tokens.length : 0;
        }

        async function emitHardSegment(text, result, options = {}) {
          const cleaned = (text || '').trim();
          if (!cleaned) {
            return null;
          }

          // Determine if this segment continues a sentence or starts a new one
          const lastChar = cleaned[cleaned.length - 1];
          const endsWithTerminalPunct = lastChar === '.' || lastChar === '!' || lastChar === '?';

          // SDK finals ending with .!? are complete sentences (not continuations)
          const isContinuation = !endsWithTerminalPunct;

          // Each hard segment gets its own unique utteranceId
          // (Don't reuse currentUtteranceId - that's for partials only)
          const newUtteranceId = generateUtteranceId();

          const utteranceId = await postPatch(cleaned, 'hard', result, {
            meta: {
              ...options.meta,
              isContinuation  // Add continuation flag for translation
            },
            targets: options.targets,
            srcLang: options.srcLang,
            utteranceId: newUtteranceId
          });

          if (!utteranceId) {
            log('Hard segment POST failed', { length: cleaned.length });
            return null;
          }

          // Always reset utteranceId after emitting a hard segment
          // (Next partial will start a new utterance)
          state.currentUtteranceId = null;
          state.rev = 0;
          state.lastSoftText = '';
          state.lastSoftEmitAt = 0;

          return utteranceId;
        }

        async function postPatch(text, stage, result, options = {}) {
          if (!text) {
            return null;
          }

          // If caller provided explicit utteranceId (e.g., for hard finals), use rev: 0
          // Otherwise, use/create currentUtteranceId and increment rev
          let utteranceId, rev;

          if (options.utteranceId) {
            // Explicit utteranceId provided (hard segment) - start fresh at rev: 0
            utteranceId = options.utteranceId;
            rev = 0;
          } else {
            // No explicit utteranceId (soft patch) - use/create current and increment
            if (!state.currentUtteranceId) {
              state.currentUtteranceId = generateUtteranceId();
              state.rev = 0;
            }
            utteranceId = state.currentUtteranceId;
            rev = state.rev;
          }

          const targets = Array.isArray(options.targets) ? options.targets : state.targetLangs;

          const patch = {
            utteranceId,
            stage,
            op: 'replace',
            rev,
            text,
            srcLang: options.srcLang ?? state.patchSourceLang ?? state.sourceLang,
            isFinal: stage === 'hard',
            ts: computeTimestamps(result)
          };

          if (options.meta && typeof options.meta === 'object') {
            patch.meta = options.meta;
          }

          const payload = {
            roomId: state.roomId,
            targets,
            patch
          };

          try {
            const res = await fetch('/api/segments', {
              method: 'POST',
              headers: { 'Content-Type': 'application/json' },
              body: JSON.stringify(payload)
            });

            if (!res.ok) {
              let body = null;
              try {
                body = await res.json();
              } catch (parseErr) {
                body = await res.text().catch(() => null);
              }
              log('Patch POST failed', { status: res.status, body });
              return null;
            }

            // Check response body for dropped segments (server returns HTTP 202 with ok: false for dropped segments)
            try {
              const body = await res.json();

              // Server returns { ok: false, stale: true, empty?: true } when segment is dropped
              if (body && (body.ok === false || body.stale === true)) {
                const reason = body.empty ? 'only filler words' : 'stale version';
                const droppedText = patch.text?.substring(0, 100) || '(empty)';

                log('‚ö†Ô∏è  Segment DROPPED by server', {
                  reason,
                  unitId: patch.utteranceId || patch.unitId,
                  text: droppedText,
                  isEmpty: Boolean(body.empty),
                  isStale: Boolean(body.stale),
                  stage: patch.stage
                });

                // Don't increment revision for dropped segments
                return null;
              }
            } catch (parseErr) {
              // Response body wasn't JSON or couldn't be parsed - assume success
              log('Could not parse segment response (assuming success)', {
                status: res.status,
                error: parseErr?.message
              });
            }

            // Only increment revision if we're using currentUtteranceId (soft patches)
            // Hard segments get new utteranceId each time
            if (!options.utteranceId) {
              state.rev++;
            }

            return utteranceId;
          } catch (err) {
            log('Patch POST error', { error: err?.message });
          }
          return null;
        }

        function renderPatch(patch) {
          const utterId = patch.utteranceId || patch.unitId;
          if (!utterId) {
            return;
          }

          if (patch.stage !== 'hard') {
            log('Soft patch (hidden)', {
              utteranceId: utterId,
              text: patch.text?.substring(0, 40),
              rev: patch.rev ?? patch.version
            });
            return;
          }

          // Get or create the current paragraph
          let currentParagraph = transcriptList.lastElementChild;

          // Create new paragraph if:
          // 1. No paragraph exists, OR
          // 2. Long pause (5s) AND previous sentence ended with terminal punctuation
          const now = Date.now();
          const lastUpdate = currentParagraph?.dataset.lastUpdate ? parseInt(currentParagraph.dataset.lastUpdate) : 0;
          const timeSinceLastUpdate = now - lastUpdate;

          let shouldCreateNewParagraph = false;
          if (!currentParagraph) {
            shouldCreateNewParagraph = true;
          } else if (timeSinceLastUpdate > 5000) {
            // Check if previous text ended with terminal punctuation
            const previousText = currentParagraph.textContent.trim();
            if (previousText) {
              const lastChar = previousText[previousText.length - 1];
              const endsWithTerminalPunct = lastChar === '.' || lastChar === '!' || lastChar === '?';
              shouldCreateNewParagraph = endsWithTerminalPunct;

              if (!endsWithTerminalPunct) {
                log('üìÑ Skipping new paragraph (mid-sentence)', {
                  timeSinceLastUpdate: Math.round(timeSinceLastUpdate),
                  previousEnded: lastChar
                });
              }
            } else {
              // Empty paragraph - create new one
              shouldCreateNewParagraph = true;
            }
          }

          if (shouldCreateNewParagraph) {
            currentParagraph = document.createElement('li');
            currentParagraph.dataset.lastUpdate = now;
            transcriptList.appendChild(currentParagraph);
          } else {
            currentParagraph.dataset.lastUpdate = now;
          }

          // Append text with space separator
          const currentText = currentParagraph.textContent;
          let textToAppend = patch.text;

          // Smart capitalization: Only capitalize if starting a new sentence
          if (textToAppend) {
            let shouldCapitalize = false;
            const firstChar = textToAppend.charAt(0);
            const isAlreadyCapitalized = firstChar === firstChar.toUpperCase();

            if (!currentText) {
              // First segment in paragraph ‚Üí always capitalize
              shouldCapitalize = !isAlreadyCapitalized;
            } else {
              // Check if previous text ends with terminal punctuation (.!?)
              // Handle edge cases: quotes/parens after punctuation
              const trimmedCurrent = currentText.trim();

              // Find the last meaningful punctuation (ignore trailing quotes/parens/spaces)
              let lastMeaningfulChar = '';
              for (let i = trimmedCurrent.length - 1; i >= 0; i--) {
                const char = trimmedCurrent[i];
                if (char !== ' ' && char !== '"' && char !== "'" && char !== ')' && char !== ']') {
                  lastMeaningfulChar = char;
                  break;
                }
              }

              const endsWithTerminal = lastMeaningfulChar === '.' || lastMeaningfulChar === '!' || lastMeaningfulChar === '?';
              shouldCapitalize = endsWithTerminal && !isAlreadyCapitalized;
            }

            if (shouldCapitalize) {
              textToAppend = textToAppend.charAt(0).toUpperCase() + textToAppend.slice(1);
              log('üî§ Capitalizing segment (new sentence)', {
                previousEnded: currentText ? currentText.trim().slice(-1) : '(none)',
                segment: textToAppend.substring(0, 30)
              });
            } else {
              log('üî§ Preserving SDK capitalization', {
                wasCapitalized: isAlreadyCapitalized,
                previousEnded: currentText ? currentText.trim().slice(-10) : '(none)',
                segment: textToAppend.substring(0, 30)
              });
            }
          }

          currentParagraph.textContent = currentText
            ? `${currentText} ${textToAppend}`
            : textToAppend;

          currentParagraph.dataset.stage = patch.stage;

          // Track this utterance
          state.itemsByUnit.set(utterId, currentParagraph);

          log('Hard patch appended to paragraph', {
            utteranceId: utterId,
            text: patch.text?.substring(0, 50),
            paragraphLength: currentParagraph.textContent.length,
            rev: patch.rev ?? patch.version
          });

          // Cleanup old paragraphs to prevent browser slowdown
          cleanupOldParagraphs();
        }

        function cleanupOldParagraphs() {
          const MAX_PARAGRAPHS = 20; // Keep last 20 paragraphs (live streaming focus)
          const paragraphs = transcriptList.querySelectorAll('li');

          if (paragraphs.length > MAX_PARAGRAPHS) {
            const removeCount = paragraphs.length - MAX_PARAGRAPHS;
            for (let i = 0; i < removeCount; i++) {
              paragraphs[i].remove();
            }
            log('Cleaned up old paragraphs', {
              removed: removeCount,
              remaining: MAX_PARAGRAPHS
            });
          }
        }

        function attachWebSocket() {
          if (state.ws) {
            state.ws.close();
          }
          const wsUrl = `${location.protocol === 'https:' ? 'wss' : 'ws'}://${location.host}/ws?room=${
            state.roomId
          }&role=speaker&lang=source`;
          const socket = new WebSocket(wsUrl);
          socket.onopen = () => log('Speaker WS connected');
          socket.onclose = () => log('Speaker WS closed');
          socket.onerror = (event) => log('Speaker WS error', { event });
          socket.onmessage = (event) => {
            try {
              const message = JSON.parse(event.data);
              if (message.type === 'patch') {
                renderPatch(message.payload);
              } else if (message.type === 'watchdog') {
                log('Watchdog ping', message.payload);
              } else if (message.type === 'reset') {
                state.itemsByUnit.clear();
                transcriptList.innerHTML = '';
                log('Transcript reset by server.');
              }
            } catch (err) {
              log('WS message parse failed', { error: err?.message });
            }
          };
          state.ws = socket;
        }

        function startHeartbeat() {
          stopHeartbeat();
          const interval = state.heartbeatIntervalMs || 2500;
          state.heartbeatTimer = setInterval(() => {
            if (state.ws && state.ws.readyState === WebSocket.OPEN) {
              state.ws.send(JSON.stringify({ type: 'heartbeat', payload: { pcm: true } }));
            }
          }, interval);
        }

        function stopHeartbeat() {
          if (state.heartbeatTimer) {
            clearInterval(state.heartbeatTimer);
            state.heartbeatTimer = null;
          }
        }

        function clearTokenRefresh() {
          if (state.tokenTimer) {
            clearInterval(state.tokenTimer);
            state.tokenTimer = null;
          }
        }

        function scheduleTokenRefresh() {
          clearTokenRefresh();
          const config = state.config || DEFAULT_CONFIG;
          const refreshWindow = config.speechTokenRefreshMs || DEFAULT_CONFIG.speechTokenRefreshMs;
          const interval = Math.max(refreshWindow - 10000, 60000);
          state.tokenTimer = setInterval(async () => {
            if (!state.recognizer) {
              return;
            }
            try {
              const { token } = await acquireToken({ quiet: true });
              if (token) {
                state.recognizer.authorizationToken = token;
              }
            } catch (err) {
              log('Token refresh failed', { error: err?.message });
            }
          }, interval);
        }

        async function acquireToken({ quiet = false } = {}) {
          const res = await fetch('/api/speech/token', { method: 'POST' });
          if (!res.ok) {
            throw new Error(`Token request failed with status ${res.status}`);
          }
          const body = await res.json();
          if (!body?.token || !body?.region) {
            throw new Error('Token response missing token/region.');
          }
          if (!quiet) {
            log('Token acquired', { region: body.region, expiresIn: body.expiresInSeconds });
          }
          return body;
        }

        async function start() {
          startBtn.disabled = true;
          stopBtn.disabled = false;
          state.roomId = document.getElementById('roomId').value.trim() || 'demo-room';
          const inputSourceLang = document.getElementById('sourceLang').value.trim() || 'en-US';
          state.sourceLang = inputSourceLang;
          state.patchSourceLang = inputSourceLang;
          state.targetLangs = parseTargetLangs(document.getElementById('targetLangs').value);

          // Read selected recognition mode from UI
          const selectedModeRadio = document.querySelector('input[name="recMode"]:checked');
          state.recognitionMode = selectedModeRadio ? selectedModeRadio.value : 'dictation';

          state.sessionId = crypto.randomUUID();
          state.currentUtteranceId = null;
          state.rev = 0;
          // Partials disabled - removed state resets for: lastSoftEmitAt, lastSoftText, lastPartialFull, prefix
          state.itemsByUnit.clear();
          transcriptList.innerHTML = '';

          try {
            const config = await loadConfig();
            setStatus('Authorising‚Ä¶');
            const { token, region } = await acquireToken();
            state.tokenRegion = region;

            const speechConfig = SpeechSDK.SpeechConfig.fromAuthorizationToken(token, region);
            const useAutoDetect =
              inputSourceLang.toLowerCase() === 'auto' && state.autoDetectLangs.length >= 2;
            if (useAutoDetect) {
              state.sourceLang = state.autoDetectLangs[0] || 'en-US';
              state.patchSourceLang = undefined;
            } else {
              state.sourceLang = inputSourceLang;
              state.patchSourceLang = state.sourceLang;
            }
            if (!useAutoDetect) {
              speechConfig.speechRecognitionLanguage = state.sourceLang;
            }

            // Set recognition mode from UI selection (overrides .env config)
            const recognitionMode = state.recognitionMode || 'balanced';
            log(`Using recognition mode: ${recognitionMode} (user-selected)`);

            if (recognitionMode === 'dictation' || recognitionMode === 'balanced') {
              speechConfig.enableDictation();
              if (recognitionMode === 'dictation') {
                log('Dictation mode: Standard punctuation, semantic segmentation');
              } else {
                log('Balanced mode: Semantic segmentation + moderately aggressive VAD');
              }
            } else if (recognitionMode === 'conversation') {
              speechConfig.setProperty(
                SpeechSDK.PropertyId.SpeechServiceConnection_Mode,
                'conversation'
              );
              log('Conversation mode: Fast VAD, aggressive silence thresholds');
            }

            // Set language detection priority to Accuracy (when using auto-detect)
            if (useAutoDetect) {
              speechConfig.setProperty(
                SpeechSDK.PropertyId.SpeechServiceConnection_LanguageIdPriority,
                "Accuracy"
              );
              log('Language detection priority set to Accuracy');
            }

            speechConfig.outputFormat = SpeechSDK.OutputFormat.Detailed;

            const stablePartials = config.stablePartials ?? DEFAULT_CONFIG.stablePartials;
            const initialSilenceMs = config.initialSilenceMs ?? DEFAULT_CONFIG.initialSilenceMs;

            // Configure VAD settings based on recognition mode
            let segmentationSilenceMs, endSilenceMs;
            if (recognitionMode === 'conversation') {
              // Conversation mode: Super aggressive VAD for fast response
              segmentationSilenceMs = 150;  // Fire after 150ms pause
              endSilenceMs = 150;            // End after 150ms trailing silence
            } else if (recognitionMode === 'balanced') {
              // Balanced mode: Moderately aggressive VAD with semantic segmentation
              segmentationSilenceMs = 200;  // Fire after 200ms pause (faster than dictation's 300ms)
              endSilenceMs = 150;            // End after 150ms trailing silence (faster than dictation's 200ms)
            } else {
              // Dictation mode: Standard VAD (semantic handles segmentation)
              segmentationSilenceMs = config.segmentationSilenceMs ?? DEFAULT_CONFIG.segmentationSilenceMs;
              endSilenceMs = config.endSilenceMs ?? DEFAULT_CONFIG.endSilenceMs;
            }

            speechConfig.setProperty(
              SpeechSDK.PropertyId.SpeechServiceResponse_StablePartialResultThreshold,
              String(stablePartials)
            );
            speechConfig.setProperty(
              SpeechSDK.PropertyId.Speech_SegmentationSilenceTimeoutMs,
              String(segmentationSilenceMs)
            );
            speechConfig.setProperty(
              SpeechSDK.PropertyId.SpeechServiceConnection_InitialSilenceTimeoutMs,
              String(initialSilenceMs)
            );
            speechConfig.setProperty(
              SpeechSDK.PropertyId.SpeechServiceConnection_EndSilenceTimeoutMs,
              String(endSilenceMs)
            );
            speechConfig.setProperty(
              SpeechSDK.PropertyId.SpeechServiceResponse_RequestSentenceBoundary,
              'true'
            );
            speechConfig.setProperty(
              SpeechSDK.PropertyId.SpeechServiceResponse_RequestWordBoundary,
              'true'
            );
            speechConfig.setProperty(
              SpeechSDK.PropertyId.SpeechServiceResponse_RequestPunctuationBoundary,
              'true'
            );

            // Enable Azure's cloud-side audio processing (noise suppression, echo cancellation)
            // This helps VAD detect silence even with background noise (breathing, keyboard, etc.)
            speechConfig.setProperty(
              SpeechSDK.PropertyId.SpeechServiceConnection_EnableAudioProcessing,
              'true'
            );

            // Configure segmentation strategy based on mode
            if (recognitionMode === 'dictation' || recognitionMode === 'balanced') {
              // Dictation/Balanced: Use semantic segmentation (fires on punctuation boundaries)
              // Note: In JavaScript SDK, semantic still accumulates context (waits 5-10s)
              // Balanced mode combines semantic with more aggressive VAD for faster response
              speechConfig.setProperty(
                SpeechSDK.PropertyId.Speech_SegmentationStrategy,
                'Semantic'
              );
              log('Semantic segmentation enabled (punctuation-based)');
            } else {
              // Conversation: Use default VAD-based segmentation (fires on silence only)
              // No need to set property - default behavior
              log('Using default VAD segmentation (silence-based)');
            }

            // NOTE: Speech_SegmentationMaximumTimeMs is not supported in JavaScript SDK
            // Causes WebSocket error 1007 "Could not validate speech context"
            // This property exists in C#/Python but not browser JS

            // Log SDK configuration for debugging
            log('Azure SDK configured', {
              recognitionMode,
              stablePartials,
              segmentationSilenceMs,
              initialSilenceMs,
              endSilenceMs,
              maxUtteranceDurationMs: config.maxUtteranceDurationMs
            });

            // Use default microphone input (fromStreamInput is broken - causes hallucinated transcriptions)
            log('Creating audio config from default microphone');
            const audioConfig = SpeechSDK.AudioConfig.fromDefaultMicrophoneInput();

            let recognizer;
            if (useAutoDetect) {
              const candidates = state.autoDetectLangs.slice(0, 4);
              const autoDetectConfig = SpeechSDK.AutoDetectSourceLanguageConfig.fromLanguages(
                candidates
              );
              recognizer =
                typeof SpeechSDK.SpeechRecognizer.FromConfig === 'function'
                  ? SpeechSDK.SpeechRecognizer.FromConfig(
                      speechConfig,
                      autoDetectConfig,
                      audioConfig
                    )
                  : new SpeechSDK.SpeechRecognizer(speechConfig, autoDetectConfig, audioConfig);
            } else {
              recognizer = new SpeechSDK.SpeechRecognizer(speechConfig, audioConfig);
            }
            state.recognizer = recognizer;
            if (state.phraseList && typeof state.phraseList.clear === 'function') {
              state.phraseList.clear();
            }
            if (Array.isArray(state.phraseHints) && state.phraseHints.length) {
              try {
                const phraseList = SpeechSDK.PhraseListGrammar.fromRecognizer(recognizer);
                state.phraseHints.slice(0, 200).forEach((hint) => phraseList.addPhrase(hint));
                state.phraseList = phraseList;
              } catch (err) {
                log('Failed to apply phrase hints', { error: err?.message });
              }
            }

            attachWebSocket();
            startHeartbeat();

            recognizer.recognizing = (_, event) => {
              // PARTIALS DISABLED: With semantic segmentation, SDK finals fire so quickly (2-3s)
              // that partials are unnecessary noise. Only SDK finals are emitted.

              // Optional: Keep minimal logging for debugging (sampling to avoid spam)
              if (Math.random() < 0.05) {
                const partial = event?.result?.text || '';
                log('Recognizing (partial - not emitted)', {
                  text: partial.substring(0, 40) + '...',
                  len: partial.length
                });
              }
            };

            recognizer.recognized = (_, event) => {
              // Map reason code to text (NoMatch=0, RecognizedSpeech=3, Canceled=2)
              const reasonText = event?.result?.reason === 0
                ? 'NoMatch'
                : event?.result?.reason === 3
                ? 'RecognizedSpeech'
                : event?.result?.reason === 2
                ? 'Canceled'
                : `Unknown(${event?.result?.reason})`;

              log('Recognized (final) event', {
                reason: reasonText,
                text: event?.result?.text?.substring(0, 50) || '(empty)'
              });

              // Log NoMatch only if it's unexpected (not just silence)
              if (event?.result?.reason === 0 && event?.result?.text !== '') {
                log('‚ö†Ô∏è NoMatch', { lang: state.sourceLang });
              }

              if (!event || !event.result) {
                return;
              }
              if (event.result.reason === SpeechSDK.ResultReason.RecognizedSpeech) {
                const finalText = (event.result.text || '').trim();
                if (!finalText) {
                  // Empty final - just skip it
                  return;
                }
                const activeConfig = state.config || DEFAULT_CONFIG;
                const delay = Math.max(
                  Number(activeConfig.finalDebounceMs ?? DEFAULT_CONFIG.finalDebounceMs) || 0,
                  0
                );

                const now = Date.now();

                // Add this final to the pending queue (accumulate instead of replace)
                state.pendingFinals.push({
                  text: finalText,
                  result: event.result,
                  timestamp: now
                });

                // Track batch start time (for timeout safeguard)
                if (state.pendingFinals.length === 1) {
                  state.finalBatchStartedAt = now;
                }

                // Clear existing timer and start a new debounce window
                clearTimeout(state.finalTimer);
                state.finalTimer = null;

                // SAFEGUARD 1: Force emit if batch size limit reached
                // Prevents unbounded accumulation during continuous speech
                const MAX_BATCH_TIMEOUT_MS = 500; // Absolute max time to hold finals
                const batchAge = now - state.finalBatchStartedAt;
                const shouldForceEmit =
                  state.pendingFinals.length >= state.maxBatchSize ||
                  batchAge >= MAX_BATCH_TIMEOUT_MS;

                if (shouldForceEmit) {
                  log(`üö® Force emitting batch (size: ${state.pendingFinals.length}, age: ${batchAge}ms)`);
                }

                const emitAllPendingFinals = () => {
                  // Get all queued finals
                  const finalsToEmit = state.pendingFinals;
                  state.pendingFinals = [];  // Clear queue
                  state.finalBatchStartedAt = 0;  // Reset batch timer

                  if (!finalsToEmit.length) {
                    state.finalTimer = null;
                    return;
                  }

                  log(`üì¶ Emitting ${finalsToEmit.length} batched final(s)`, {
                    count: finalsToEmit.length,
                    batchAge: finalsToEmit.length ? Date.now() - finalsToEmit[0].timestamp : 0
                  });

                  // With semantic segmentation, SDK finals are atomic sentences (no overlap/dedup needed)
                  // Combine all finals' text with space separator
                  const combinedText = finalsToEmit
                    .map(final => final.text.trim())
                    .filter(Boolean)
                    .join(' ');

                  // Split combined text into sentences
                  const split = combinedText ? splitSentences(combinedText) : [];
                  const segments = split.length ? split : combinedText ? [combinedText] : [];

                  // Emit each sentence (using the first final's result for metadata)
                  let totalEmitted = 0;
                  const firstResult = finalsToEmit[0]?.result;
                  for (const sentence of segments) {
                    if (sentence) {
                      emitHardSegment(sentence, firstResult);
                      totalEmitted += 1;
                    }
                  }

                  // Partials disabled - removed state resets
                  state.finalTimer = null;
                  setTelemetry(
                    totalEmitted
                      ? `Hard ‚úîÔ∏é ¬∑ ${totalEmitted} segment${totalEmitted > 1 ? 's' : ''}`
                      : 'Hard ‚úîÔ∏é'
                  );
                };

                // Emit immediately if force conditions met, otherwise debounce
                if (shouldForceEmit || delay === 0) {
                  emitAllPendingFinals();
                } else {
                  // Wait for debounce window, then emit ALL accumulated finals
                  state.finalTimer = setTimeout(() => {
                    state.finalTimer = null;
                    emitAllPendingFinals();
                  }, delay);
                }
              } else if (event.result.reason === SpeechSDK.ResultReason.NoMatch) {
                log('No speech match', { reason: event.result.noMatchDetails });
              }
            };

            recognizer.canceled = (_, event) => {
              setStatus('Canceled');
              log('Recognition canceled', { reason: event.reason, error: event.errorDetails });
            };

            recognizer.sessionStarted = () => {
              setStatus('Listening');
              log('Session started');
            };

            recognizer.sessionStopped = () => {
              setStatus('Stopped');
              log('Session stopped');
              stop();
            };

            recognizer.speechStartDetected = (_, event) => {
              log('Speech start detected!', { offset: event?.offset });
            };

            recognizer.speechEndDetected = (_, event) => {
              log('Speech end detected', { offset: event?.offset });
            };

            recognizer.startContinuousRecognitionAsync(
              () => {
                setStatus('Listening');
                log('Recognition started', {
                  lang: state.sourceLang,
                  targets: state.targetLangs
                });
                scheduleTokenRefresh();
              },
              (err) => {
                log('Failed to start recognition', { error: err?.message });
                stop();
              }
            );
          } catch (err) {
            log('Start failed', { error: err?.message });
            stop();
          }
        }

        function stop() {
          clearTokenRefresh();
          stopHeartbeat();
          clearTimeout(state.finalTimer);
          state.finalTimer = null;
          // Partials disabled - removed state resets for: lastSoftEmitAt, lastSoftText, lastPartialFull, prefix
          if (state.recognizer) {
            try {
              state.recognizer.stopContinuousRecognitionAsync(() => {
                log('Recognition stopped');
              });
            } catch (err) {
              log('Error stopping recognizer', { error: err?.message });
            }
            state.recognizer = null;
          }
          if (state.ws) {
            state.ws.close(1000, 'Stopping');
            state.ws = null;
          }
          if (state.phraseList && typeof state.phraseList.clear === 'function') {
            state.phraseList.clear();
          }
          state.phraseList = null;
          state.patchSourceLang = null;
          state.tokenRegion = null;
          startBtn.disabled = false;
          stopBtn.disabled = true;
          setStatus('Idle');
          setTelemetry('‚Äî');
        }

        // Enumerate and populate microphone devices
        async function populateMicrophoneList() {
          try {
            const micSelect = document.getElementById('micSelect');

            // Request microphone permission first to get device labels
            await navigator.mediaDevices.getUserMedia({ audio: true })
              .then(stream => stream.getTracks().forEach(track => track.stop()));

            const devices = await navigator.mediaDevices.enumerateDevices();
            const audioInputs = devices.filter(device => device.kind === 'audioinput');

            // Clear existing options except the default
            micSelect.innerHTML = '<option value="">Default microphone</option>';

            // Add each microphone as an option
            audioInputs.forEach(device => {
              const option = document.createElement('option');
              option.value = device.deviceId;
              option.textContent = device.label || `Microphone ${device.deviceId.substring(0, 8)}`;
              micSelect.appendChild(option);
            });

            log('Microphone devices enumerated', { count: audioInputs.length });
          } catch (err) {
            log('Failed to enumerate microphones', { error: err?.message });
          }
        }

        // Populate microphones on page load
        populateMicrophoneList();

        loadConfig().catch((err) => log('Config bootstrap failed', { error: err?.message }));

        startBtn.addEventListener('click', () => {
          start();
        });
        stopBtn.addEventListener('click', () => {
          stop();
        });

        copyDebugBtn.addEventListener('click', async () => {
          try {
            await navigator.clipboard.writeText(logArea.textContent);
            const originalText = copyDebugBtn.textContent;
            copyDebugBtn.textContent = 'Copied!';
            setTimeout(() => {
              copyDebugBtn.textContent = originalText;
            }, 2000);
          } catch (err) {
            alert('Failed to copy: ' + err.message);
          }
        });

        window.addEventListener('beforeunload', stop);
      })();
    </script>
  </body>
</html>
